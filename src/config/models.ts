import * as THREE from "three";
import { Model } from "@/types";

const models: Model[] = [
  {
    id: "catalina",
    title: "CATALINA",
    position: [0, -1, 0],
    componentOpenPosition: [0, -1, 0],
    scale: 0.085,
    path: `./assets/models/catalina/catalina.glb`,
    hotspots: [
      //
    ],
    components: [
      {
        path: `./assets/models/catalina/bbu.glb`,
        id: "bbu",
        title: "BBU",
        position: [0, 0, 0],
        openPosition: [0, -9, 10],
        hotspots: [
          {
            description: `Battery Backup Units.`,
            position: [-0.3, 2.85, 0.675],
            openPosition: [0.125, 3.05, 0.3],
            scale: 10,
            flipped: true,
          },
        ],
      },
      {
        path: `./assets/models/catalina/duplicate-models.glb`,
        id: "duplicate-models",
        title: "",
        position: [0, 0, 0],
        hotspots: [],
        hideFromMenu: true,
      },
      {
        path: `./assets/models/catalina/power-shelf.glb`,
        id: "power-shelf",
        title: "POWER SHELF",
        position: [0, 0, 0],
        openPosition: [0, -9, 10],
        hotspots: [
          {
            description: `Power Supply Shelf.`,
            position: [0.3, 2.75, 0.675],
            openPosition: [-0.125, 2.925, 0.3],
            scale: 10,
          },
        ],
      },
      {
        path: `./assets/models/catalina/fiber-patch-panel.glb`,
        id: "fiber-patch-panel",
        title: "FIBER PATCH PANEL",
        position: [0, 0, 0],
        openPosition: [0, -8, 10],
        hotspots: [
          {
            description: `A panel that is a central point for connecting and managing fiber optic cables in a rack.`,
            position: [0.3, 2.6375, 0.675],
            openPosition: [-0.125, 2.925, 0.3],
            scale: 10,
          },
        ],
      },
      {
        path: `./assets/models/catalina/management-controller.glb`,
        id: "management-controller",
        title: "RACK MANAGEMENT CONTROLLER",
        position: [0, 0, 0],
        openPosition: [0, -8, 10],
        hotspots: [
          {
            description: `Rack Management Controller (RMC) is a rack unit that currently primarily handles Liquid cooling flows (leaks, repair, maintenance) and power sequencing.`,
            position: [-0.1, 2.66, 0.675],
            openPosition: [0.125, 2.8, 0.3],
            scale: 10,
            flipped: true,
          },
        ],
      },
      {
        path: `./assets/models/catalina/wedge-400-c.glb`,
        id: "wedge-400-c",
        title: "WEDGE 400C",
        position: [0, 0, 0],
        openPosition: [0, -7, 10],
        hotspots: [
          {
            description: `Top of Rack Switch.`,
            position: [-0.2, 2.45, 0.675],
            openPosition: [0.125, 2.7, 0.3],
            scale: 10,
            flipped: true,
          },
        ],
      },
      {
        path: `./assets/models/catalina/management-switch.glb`,
        id: "management-switch",
        title: "RACK MANAGEMENT SWITCH",
        position: [0, 0, 0],
        openPosition: [0, 7, 10],
        hotspots: [
          {
            description: `A network device is a switch that enables connectivity between management, console, and the data plane.`,
            position: [0.25, 0.7, 0.675],
            openPosition: [-0.125, 0.7, 0.4],
            scale: 10,
          },
        ],
      },
    ],
  },
  {
    id: "orv3",
    title: "OPEN RACK v3",
    position: [0, -1, 0],
    componentOpenPosition: [0, -1, 0],
    scale: 0.8,
    path: `./assets/models/orv3/orv3.glb`,
    hotspots: [
      {
        scale: 0.29,
        rotation: [0, THREE.MathUtils.degToRad(180), 0],
        position: [0.7, 5.4, 1.2],
        title: "AC WHIP POWER CABLE",
        description: `On ORV3 the cables run directly to the power shelf whereas V2 had a separate power distribution unit. Coupled with the different lengths of cable, this offers more flexibility for you to put the power shelf anywhere in the rack.  The AC Whip Power cable will have two separate versions for EU and North America customers to allow for different voltages and currents.`,
      },
      {
        scale: 0.29,
        rotation: [0, THREE.MathUtils.degToRad(180), 0],
        position: [0, 1.75, 1.3],
        title: "BUS BAR",
        description: `The improved Bus Bar now has no fixed zones and is easily replaceable with just a few screws. The Bus Bar allows for a pluggable Blind Mate IT gear connection. This allows easier customization and servicing of parts.  It also has increased power capability and can service 48V compared to the 12V in Open rack V2`,
      },
      {
        scale: 0.29,
        position: [0.8, 2.49, 1.2],
        title: "ORV3",
        description: `Meet the latest generation of Open Rack.
Open Rack V3 has many new features that will improve how we design, configure, build, deploy and manage our IT gear. ORV3 features a new 48v power systems & distributed busbar that can support higher power densities and faster load transients, as well as an increased battery backup system providing up to 4 minutes of backup during outage events. It also supports flexible IT gear form factors for supporting OpenU & RU gear, pluggable connection to the busbar, flexible shelf placement locations, and increased rack height with support for tool-less rails to enable more gear and faster development.`,
      },
    ],
    components: [
      {
        path: `./assets/models/orv3/power-shelf.glb`,
        id: "power-shelf",
        title: "POWER SHELF",
        position: [0, 2.17, 0],
        openPosition: [0, 1.075, 1],
        hotspots: [
          {
            description: `Meet the Power Shelf. In ORV3 the Power Shelf can be installed in any OU position and isn’t bolted to the busbar, allowing for maximum rack configuration flexibility. With the increased power capability and efficiency of 48V, ORV3 delivers more total kilowatt than ORV2.`,
            position: [0.45, 0.1, 0.9],
            openPosition: [-0.25, 0.2, 0.68],
          },
        ],
      },
      {
        path: `./assets/models/orv3/bbu.glb`,
        id: "bbu",
        title: "BATTERY BACK UP",
        position: [0, 2.075, 0],
        openPosition: [0, 1.075, 1],
        hotspots: [
          {
            description: `The increased performance Battery Backup (BBU) module has a 4 minute capacity compared to 90 seconds in its predecessor. It also has more capacity with 15 kilowatts of power as well as increased flexibility. Similar to the Power Shelf, you can put the BBU in any location you want to customize your needs.`,
            position: [-0.1, -0.1, 0.9],
            openPosition: [0.25, 0.25, 0.62],
            flipped: true,
          },
        ],
      },
    ],
  },
  {
    path: `./assets/models/grand-teton/grand-teton.glb`,
    id: "grand-teton",
    title: "GRAND TETON",
    position: [0.2, 0, 0],
    rotation: [THREE.MathUtils.degToRad(20), THREE.MathUtils.degToRad(-20), 0],
    componentOpenPosition: [0, -0.3, -0.75],
    scale: 1.3,
    maxDistance: 3,
    minDistance: 1.3,
    hotspots: [
      {
        title: `CHASSIS`,
        description: `Compared to Zion, the Grand Teton monolithic chassis has eliminated eternal cables and replaced them with blind mate connectors allowing for increased reliability. A standard ORv3 rack can accommodate two 8OU Grand Tetons.  Unlike its predecessor, Zion, the Grand Teton chassis has a fan wall made up of 16 rear serviceable fans.  Finally, in total the Chassis has 33 field replaceable units`,
        position: [-0.7, 0.15, 1],
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
        scale: 0.3,
        flipped: true,
      },
    ],
    components: [
      {
        path: `./assets/models/grand-teton/arrowhead-pools.glb`,
        id: "arrowhead-pools",
        title: "ARROWHEAD POOLS",
        position: [0, 0, 0.1],
        openPosition: [0, 0, 0.75],
        hotspots: [
          {
            description: `Arrowhead Pool is our next generation GPU tray, it features 8 GPUs to enable both training and inference applications within the data center. As it weighs over 41kilos, the tray is on rails, has a ‘hard stop’ and safety latches to ensure safe serviceability for on-site technicians.`,
            position: [-0, 0.1, 0.575],
            openPosition: [-0.175, -0.1, 0.4],
            rotation: [0, THREE.MathUtils.degToRad(5), 0],
          },
        ],
      },
      {
        path: `./assets/models/grand-teton/storm-point.glb`,
        id: "storm-point",
        title: "STORM POINT",
        position: [0, 0.28, 0.1],
        openPosition: [0, 0, 0.75],
        hotspots: [
          {
            description: `Storm Point is our next generation CPU tray. It is a two socket system with 39 field replaceable units (FRUs) including:
- The DIMMs
- The CPUs
- And the TSFF NICs (tall small form factor network interface card)

This new CPU tray has increased memory, bandwidth and performance compared to its predecessor Angels Landing.`,
            position: [0.1, 0.1, 0.55],
            openPosition: [-0.175, -0.1, 0.25],
            rotation: [0, THREE.MathUtils.degToRad(20), 0],
          },
        ],
      },
      {
        path: `./assets/models/grand-teton/cascade-creek.glb`,
        id: "cascade-creek",
        title: "CASCADE CREEK",
        position: [0, -0.01, 0],
        openPosition: [0, 0, 0.75],
        hotspots: [
          {
            description: `Cascade Creek is our Switch Tray. it is comprised of;
- 8 x RDMA NICs
- And up to 16 x E1.S Drives
Bringing the total number of FRUs to 25. Both the NICs and E1.S Drives are front serviceable, and hot-swappable enabling faster serviceability for on-site technicians.`,
            position: [0.1, 0.35, 0.7],
            openPosition: [0.16, -0.1, 0.25],
            rotation: [0, THREE.MathUtils.degToRad(20), 0],
            flipped: true,
          },
        ],
      },
    ],
  },
  {
    path: `./assets/models/blind-mate-interfaces/blind-mate-interfaces.glb`,
    id: "blind-mate-interfaces",
    title: "BLIND MATE INTERFACES",
    scale: 0.75,
    position: [0, -0.9, 0],
    componentOpenPosition: [0, -0.9, 0],
    rotation: [0, 0, 0],
    hotspots: [
      {
        title: `BMQC`,
        description: `The blind mate quick connector valves are the interface between manifold hoses and IT gear cold plate loops. These connectors automatically adjust for misalignment between the plug side on the IT gear and the socket side on the manifolds. The plug side has the capability of moving and rotating to take-up the misalignment during mating. The valves also have extended working range.  The valves automatically open and close as they mate and de-mate with no manual interaction required. These valves are being developed in the OCP ORv3 Blind Mate Interfaces Group.`,
        position: [0.15, 2.3, 1],
        scale: 0.4,
      },
      {
        title: `LIQUID COOLING SUPPORT BRACKETS`,
        description: `ORV3 was designed with liquid cooling in mind. The standard ORV3 frame can be converted into a liquid cooling capable solution by installing a bolt-on support bracket kit. This kit stiffens the frame to account for the added forces induced from fluid pressures and valve spring forces, as well as provides the interfaces for mounting the blind mate manifolds to the rack.  The kit contains front web stiffener brackets, a front cross-plate stiffener, two lower manifold interface brackets, two upper manifold interface brackets, and two mid-rack manifold interface brackets.`,
        position: [-0.5, 1.6, 1],
        scale: 0.4,
      },
    ],
    components: [
      {
        path: `./assets/models/blind-mate-interfaces/blind-mate-chassis.glb`,
        id: "blind-mate-chassis",
        title: "BLIND MATE CHASSIS",
        position: [0, 1.63, -0.06],
        openPosition: [0, 1.075, 1],
        hotspots: [
          {
            description: `Blind mate chassis is a reference design for future IT gear using blind mating liquid cooling and to provide functional design during development. There are no manual hose connections or manifolds on the front of racks. Instead, blind mate chassis has quick connect plug valves mounted at the rear for mating directly into sockets mounted on the blind mate manifold. Within the chassis, there’s a theoretical liquid cooling passive cold plate loop (PCL) connecting to an internal manifold, which terminates into the hoses that connect to the quick connect plugs at the rear of the chassis. The injector handles are located at the front of the chassis, which have long travel and provide mechanical advantage to overcome the fluid and spring forces to mate the chassis into the rack.`,
            position: [0, -0.1, 0.85],
            openPosition: [-0.15, 0.3, 0.5],
            scale: 1.2,
          },
        ],
      },
      {
        path: `./assets/models/blind-mate-interfaces/manifold.glb`,
        id: "manifold",
        title: "BLIND MATE MANIFOLD",
        position: [0, 0, 0],
        openPosition: [0, 0, -0.5],
        hotspots: [
          {
            description: `The blind mate manifold provides the interface between the IT Gear and the facility fluid cooling & distribution source. The blind mate manifold consists of the manifold tube, the blind mate sockets, an optional air vent, and an inlet or outlet port with hoses to connect to fluid source and return. The blind mate manifolds are being developed in the OCP ORv3 Blind Mate Interfaces Group.`,
            position: [0.2, 2, 0.7],
            rotation: [0, THREE.MathUtils.degToRad(180), 0],
            openPosition: [0.2, 2, 0.7],
            flipped: true,
            scale: 1.2,
          },
        ],
      },
    ],
  },
  {
    path: `./assets/models/liquid-cooling-cart/liquid-cooling-cart.glb`,
    id: "liquid-cooling-cart",
    title: "LIQUID COOLING CART",
    rotation: [0, THREE.MathUtils.degToRad(-60), 0],
    position: [0, -0.6, 0],
    components: [],
    scale: 0.8,
    maxDistance: 3,
    minDistance: 2,
    hotspots: [
      {
        title: `OPERATING PANEL`,
        description: `The panel utilizes an array of pressure switches to operate the cart functions. This includes: fill, drain, compressed air, power, and emergency shutoff feature. The cart will automatically stop pumping fluid once the AALC rack is filled. It then alerts the operator of a complete procedure, by blinking and alerts of an error by maintaining a red color.`,
        position: [-0.8, 0.95, 0.5],
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        scale: 0.5,
      },
      {
        title: `MANIFOLD MOUNT`,
        description: `This mount is used for servicing of the manifold on both Blindmate and AALC racks. It carries the manifold from storage, to the data hall while containing any potential fluid leaks`,
        position: [0, 0.6, 0.5],
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        scale: 0.5,
      },
      {
        title: `EXTENDED SHELF`,
        description: `This shelf mimics what is on the standard crash carts in the data center, and provides additional workspace for the DC technicians`,
        position: [0, 1.75, 0.5],
        flipped: true,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        scale: 0.5,
      },
      {
        title: `HOSES CABINET`,
        description: `The auxiliary hoses cabinet enables the flexibility connectivity of the cart with the AALC rack, external fluid tank, and compressed air connections. This allows for new connections to be added in the future.`,
        position: [-0.4, 1.2, 0.6],
        rotation: [0, THREE.MathUtils.degToRad(90 + 180), 0],
        flipped: true,
        scale: 0.5,
      },
      {
        title: `FLUID MANAGEMENT`,
        description: `In the liquid cooling cart cabinet, you will find the new coolant reservoir, used coolant reservoir and a fluid pump. The pump allows you to top-off the rack in 3 minutes or less.  This system is capable of storing 50 liters of coolant per tank (100 liters in total), and given the weight of the cart it was essential to use appropriate casters and wheel assembly to enable the ease of mobility for on-site technicians.`,
        position: [0.3, 1, 0.6],
        rotation: [0, THREE.MathUtils.degToRad(90 + 180), 0],
        scale: 0.5,
      },
      {
        description: `The liquid cooling cart is Meta’s solution to servicing liquid cooled equipment in our data centers. This battery powered cart is capable of handling 50 liters of coolant at a time, and performs operations such as filling, draining, and purging the rack. By developing this cart, we enable our data center technicians to confidently interact with liquid enabled equipment in all data center variants.`,
        flipped: true,
        openPosition: [0, 0, 0],
        position: [0.25, 1.8, 0.6],
        rotation: [0, THREE.MathUtils.degToRad(90 + 180), 0],
        scale: 0.5,
      },
    ],
    animationHotspot: {
      position: [0, 1.5, 0.5],
      rotation: [0, THREE.MathUtils.degToRad(90), 0],
      scale: 0.5,
      playLabel: "OPEN DRAWER",
      stopLabel: "CLOSE DRAWER",
    },
  },
  {
    path: `./assets/models/grand-canyon/grand-canyon.glb`,
    id: "grand-canyon",
    title: "GRAND CANYON",
    scale: 0.9,
    position: [0.1, -0, 0],
    rotation: [THREE.MathUtils.degToRad(20), THREE.MathUtils.degToRad(-20), 0],
    components: [],
    maxDistance: 3,
    minDistance: 1.5,
    hotspots: [
      {
        scale: 0.25,
        title: `REAR FANS`,
        description: `In the rear of the chassis, we have four hot swappable fan modules. In order to improve vibration performance, these fan cages are isolated from the chassis via elastomer padding. They also feature a lower fan blade count and improved blade geometry. There is an option to mount fan louvers to reduce air leakage during fan failure and lower fan speeds.`,
        position: [0.3, 0.6, 1.4],
        rotation: [0, THREE.MathUtils.degToRad(180), 0],
        flipped: true,
      },
      {
        scale: 0.25,
        title: `BARTON SPRINGS CPUS`,
        description: `Grand Canyon features two Barton Springs cards, which are 1 socket server modules. These are top-accessible and specifically designed to maximize chassis drive density while maintaining all the necessary features. The card has one CPU, four DIMM sockets, a PCH, and an M.2 SSD for boot purposes.`,
        position: [0.75, 0.35, 1.2],
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
      },
      {
        scale: 0.25,
        title: `CHASSIS DESIGN`,
        description: `Grand Canyon features several improvements over Bryce Canyon. The power cable track has been redesigned for easier serviceability and reliability. Many new features were implemented to help with improving drive vibration dampening, such as new HDD latch materials, improved air sealing, and the addition of optional acoustic foam. The front handles were improved for easier functionality and increased reliability as well.`,
        position: [-0.8, 0.5, 1.2],
        rotation: [0, THREE.MathUtils.degToRad(180), 0],
        flipped: true,
      },
      {
        scale: 0.25,
        title: `HARD DRIVES (HDDS)`,
        description: `The system features 72 hot swappable 3.5” form factor hard drives. These drives can easily be serviced by engaging a latch and pulling out the drive from its designated location. The latch assembly is also designed to keep all drives in the unseated position in case the drive plane board (DPB) needs to be replaced.`,
        position: [-0.8, 0.2, 1.2],
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
        flipped: true,
      },
      {
        scale: 0.25,
        title: `STORAGE CONTROLLER CARDS (SCC)`,
        description: `Grand Canyon features two Storage Controller Cards (SCCs). These are top-accessible modules that contain the components necessary to translate PCIe connectivity from the server card to SAS/SATA for the HDDs. These cards are easily serviceable with the release of one ejector handle.`,
        position: [-0.1, -0.1, 1.2],
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
      },
      {
        scale: 0.25,
        description: `Grand Canyon is Meta’s next generation storage platform with improved hardware security and modularity for future upgrades of key commodities. The platform is designed to support higher density HDD without performance degradation and improves power utilization. The platform Enables large scale deployments in hyperscale environments by leveraging OpenBMC, OCP NIC, industry standard interfaces and improves serviceability.`,
        position: [0, 0.7, -0.1],
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
      },
    ],
    animationHotspot: {
      scale: 0.25,
      position: [0.15, 0.7, 1.2],
      rotation: [0, THREE.MathUtils.degToRad(0), 0],
      flipped: true,
      playLabel: "OPEN CHASSIS",
      stopLabel: "CLOSE CHASSIS",
    },
  },
  {
    scale: 2.5,
    position: [0.1, 0.1, 0],
    rotation: [THREE.MathUtils.degToRad(20), THREE.MathUtils.degToRad(-20), 0],
    path: `./assets/models/glacier-point/glacier-point.glb`,
    id: "glacier-point",
    title: "GLACIER POINT",
    maxDistance: 3,
    minDistance: 1.4,
    hotspots: [
      {
        title: `RISER CAGE`,
        description: `Now, you can view the Riser Cage with the Glacier Point v3 Board. The bottom M.2 modules as well as the 2x E1.S drives are now accessible for service. You will also see a heat sink on the Glacier Point v3 Board which is for the switch to route traffic between the expansion board and Delta Lake motherboard`,
        scale: 0.15,
        position: [-0.1, 0.025, 1.4],
        flipped: true,
      },
      {
        title: `M.2 MODULES`,
        description: `By opening the top cover, you can easily access the upper 6 M.2 Modules mounted to the Glacier Point v3 Board. This board also has a switch that allows for traffic communication between the M.2 modules and Intel Cooper Lake Host.   The latching mechanism on each side of the blade, allows you to unlock the 2OU Riser Cage from the Blade Base to access the lower M.2 Modules and two E1.S drives for Storage.`,
        scale: 0.15,
        position: [0.5, 0.325, 1],
      },
      {
        title: `SERVICEABILITY`,
        description: `By removal of the Green 2OU Air Baffle, you can access the Rear portion of the Delta Lake Motherboard. The DIMMs, CPU, Heat Sink, and Boot drive can easily be accessed for service. As you see here, this model is supporting 4 DIMMs. Up to 6 can be used with this system`,
        scale: 0.15,
        position: [-0.5, 0.325, -0.2],
        flipped: true,
      },
      {
        scale: 0.15,
        position: [-0.25, 0.525, 0.1],
        description: `Meet the latest generation of Glacier Peak.

Glacier Peak V3 allows for further expansion of the Yosemite V3 Platform. It is a M.2/Dual M.2 carrier card that supports PCIe gen4 accelerator modules and 2xE1.S SSD on the Yosemite v3 platform using a Gen4 capable PCIE switch.`,
      },
    ],
    components: [],
    animationHotspot: {
      position: [0, 0.3, 1.4],
      rotation: [0, THREE.MathUtils.degToRad(0), 0],
      scale: 0.15,
      playLabel: "LOOK INSIDE",
      stopLabel: "CLOSE",
    },
  },
  {
    path: `./assets/models/discovery-point/discovery-point.glb`,
    id: "discovery-point",
    title: "DISCOVERY POINT",
    components: [],
    scale: 2.1,
    position: [-0.1, 0.1, 0],
    rotation: [THREE.MathUtils.degToRad(15), THREE.MathUtils.degToRad(45), 0],
    maxDistance: 3,
    minDistance: 1.4,
    hotspots: [
      {
        scale: 0.2,
        position: [0, -0.2, 0.8],
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
        description: `Discovery Point is an expansion card to the Yosemite v3 Platform that provides an industry standard PCIe interface to a Delta Lake server board for more functionality through added storage, networking, or other cards.

With both an x8 and x16 PCIe Gen3 connection, the possibilities are endless!`,
      },
    ],
    animationHotspot: {
      position: [0, 0.2, 0.55],
      rotation: [0, THREE.MathUtils.degToRad(-90), 0],
      scale: 0.2,
      playLabel: "LOOK INSIDE",
      stopLabel: "CLOSE",
    },
  },
  {
    id: "noahs-ark",
    title: "AALC",
    position: [0, -0.9, 0],
    componentOpenPosition: [0, -0.9, 0],
    scale: 0.75,
    path: `./assets/models/noahs-ark/noahs-ark.glb`,
    hotspots: [
      {
        title: `LEAK DETECTION & CONTAINMENT`,
        description: `The AALC system is designed to avoid leaks through our quick connector hoses, hose clamps and extensive testing at five times the pressure the system will actually run at. Furthermore, the rubber hoses have passed the ASTM and UL standard tests.  To complement the leak prevention measures, the system has comprehensive leak detection and mitigation mechanisms. The leak sensor ropes are designed to sense liquid at the lowest latency possible. If one of the ropes senses a drip, DC alert manager sounds, the RPU pumps turn off and both racks automatically power down. The drain pans collect leaked coolant with each drain pan capable of holding 25 liters of fluid.  Meanwhile, the alarm tooling creates a trouble ticket to notify the data center service teams. Drain pans are removable for easy cleaning in order to get the system up and running as quickly as possible.`,
        position: [0.2, 0.5, 1.4],
        rotation: [0, THREE.MathUtils.degToRad(180), 0],
        flipped: true,
        scale: 0.4,
      },
      {
        title: `QUICK CONNECTORS`,
        description: `Quick connector valves are the interface between manifold hoses and IT gear cold plate loops. These connectors were designed with serviceability in mind.   Connectors will be mechanically keyed (plug for hot coolant exiting IT gear, socket for chilled coolant entering IT gear) to prevent user error. The connectors are drip free and can be connected or disconnected while the system is running. Clamps added to hose-barb connection to prevent pull out by user or under high pressure  Furthermore, in system, the same type of quick connectors are used to connect both the large and small hose connections.`,
        position: [0.25, 2.4, 1.2],
        scale: 0.4,
        flipped: true,
      },
    ],
    components: [
      {
        path: `./assets/models/noahs-ark/rpu.glb`,
        id: "rpu",
        title: "RPU",
        position: [-0.3, 0.16, 0],
        openPosition: [0, 1.075, 1.2],
        hotspots: [
          {
            description: `The RPU (Reservoir and Pumping Unit) provides the pressure head and distributes coolant to the entire Air Assisted Liquid Cooling (AALC) solution.​ The RPU also serves as the brain for AALC solution, which is responsible for control and monitoring of the system. How does it work? Cooled fluid flows into the RPU from the heat exchanger. Coolant collects in the RPU reservoir and is pulled into 2 pumps (the other 2 pumps are redundant). The pumps create pressure to push the coolant back out of the RPU. Finally the coolant flows out of the RPU into the cold manifold in front of the rack.`,
            position: [0, 0.1, 0.77],
            openPosition: [-0.15, 0.25, 0.3],
            scale: 1.4,
          },
        ],
      },
      {
        path: `./assets/models/noahs-ark/ttv.glb`,
        id: "ttv",
        title: "TTV",
        position: [0.305, 1.258, 0.09],
        openPosition: [0, 1.075, 1.2],
        hotspots: [
          {
            description: `The TTV (thermal test vehicle) chassis is the thermal load in an IT gear form factor that allows the testing of the AALC system without the use of functioning GPUs or other high thermal loads. The TTV chassis consists of block heaters that simulate the size and heat rejection from a GPU, covered by cold plates that flow chilled coolant over the heaters and then route hot coolant out of the chassis. The block heaters are variable in their thermal load so that different use cases can be simulated. The AALC system connects to the TTV via quick connectors on the front of the chassis.`,
            position: [0.05, -0.25, 0.67],
            openPosition: [0.175, 0.25, 0.3],
            scale: 1.4,
            flipped: true,
          },
        ],
      },
    ],
  },
  {
    path: `./assets/models/wedge-400c/wedge-400c.glb`,
    id: "wedge-400c",
    title: "WEDGE 400C",
    position: [0.2, 0, 0],
    rotation: [THREE.MathUtils.degToRad(20), THREE.MathUtils.degToRad(-20), 0],
    componentOpenPosition: [0, 0.5, -0.5],
    scale: 1.3,
    maxDistance: 3,
    minDistance: 2,
    hotspots: [
      {
        scale: 0.25,
        title: `FRONT PANEL PORT FEATURES`,
        description: `Wedge 400 & 400C have sixteen QSFP-DD ports to support 400 gigabits per seconds network speeds and thirty-two QSFP-56 ports to support 200 gigabits per seconds.  The rackmon provides connections to the power shelves (including PSUs and BBUs) in the rack.   In addition, There are two buttons on the left side of the front panel. The reset button is used to reset the whole system, and the LED button is used to switch different modes for showing the uplink/downlink status.`,
        position: [-0.25, 0.4, 1.2],
      },
    ],
    components: [
      {
        path: `./assets/models/wedge-400c/rack-adapter.glb`,
        id: "rack-adapter",
        title: "RACK ADAPTER",
        position: [0, 0, 0],
        openPosition: [0, -0.25, 0.5],
        hotspots: [
          {
            description: `The adapter tray for Wedge 400 & 400C is designed for integration with Open Rack V3.  When removing the system from the adapter tray, we have added a safety stop feature.  This is designed to stop the system pulling out all the way, allow the user to adjust their hold of the system before pressing to release the system all the way out of the tray.`,
            position: [-0.3, 0.25, 0.1],
            openPosition: [-0.13, 0.25, 0.25],
            rotation: [0, THREE.MathUtils.degToRad(20), 0],
            flipped: true,
          },
        ],
      },
      {
        path: `./assets/models/wedge-400c/module.glb`,
        id: "module",
        title: "MODULE",
        position: [0, 0, 0],
        openPosition: [0, -0.25, 0.5],
        hotspots: [
          {
            description: `In Wedge 400 and Wedge 400C the Switch Control Module, or SCM for short, is removable to allow for easier servicing. First remove Fans 1 & 2 then pull the green ejector handle to release the SCM. From there, you can remove the DIMM, Solid-State Drive or BMC Storage Module (SSD and BSM).`,
            position: [0, -0.07, 0.65],
            openPosition: [0.125, -0.25, 0.25],
            rotation: [0, THREE.MathUtils.degToRad(20), 0],
            flipped: true,
          },
        ],
      },
    ],
  },
  {
    path: `./assets/models/minipack2/minipack2.glb`,
    id: "minipack2",
    title: "MINIPACK2",
    scale: 1.3,
    position: [0.1, -0.25, 0],
    rotation: [THREE.MathUtils.degToRad(20), THREE.MathUtils.degToRad(-20), 0],
    components: [],
    maxDistance: 3,
    minDistance: 1.5,
    hotspots: [
      {
        scale: 0.25,
        title: `ADAPTER TRAY`,
        description: `The adapter tray is designed to allow Minipack2 to integrate with Open Rack v3.   The adapter tray features a safety stop feature that allows the Minipack2 system to be pulled out 6”, allowing the user to replace the rear Fan FRUs. The adapter tray also organizes the rear power cables that connect Minipack2 to the ORV3 busbar.`,
        rotation: [0, THREE.MathUtils.degToRad(180), 0],
        position: [-0.2, 0.15, 1.2],
      },
      {
        scale: 0.25,
        title: `SWITCH MAIN BOARD SERVICEABILITY`,
        description: `On the left column, eight ports of QSFP-DD cage have been designed to support 400G optics while all sixteen ports can support 200G QSFP-DD pluggable optics. The improved cages allow for better thermal cooling solutions for the next generation pluggable optics in the line cards.`,
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
        position: [-0.6, 0.05, 1.4],
      },
      {
        scale: 0.25,
        title: `400G READY`,
        description: `On the left column, eight ports of QSFP-DD cage have been designed to support 400G optics while all sixteen ports can support 200G QSFP-DD pluggable optics. The improved cages allow for better thermal cooling solutions for the next generation pluggable optics in the line cards.`,
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
        position: [-0.5, 0.5, 1.4],
        flipped: true,
      },
      {
        scale: 0.25,
        title: `LINE CARD EJECTOR`,
        description: `Based on feedback from the community, we improved the arm design from Minipack1 to Minipack2. The new line card ejector arm is designed for easier serviceability with a spring loaded ejector handle that pops out automatically when released and an easier to access release button.`,
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
        position: [0.2, 0.8, 1.4],
        flipped: true,
      },
      {
        scale: 0.3,
        description: `Minipack2 is Meta’s next generation fabric switch, doubling the bandwidth of Minipack1. Minipack2 is positioned in the fabric layer, connecting rack switches to each other and also forwards traffic up to higher layers of the network. By designing for more bandwidth, this allows for greater machine to machine (or east to west) traffic and machine to user (or north to south) traffic.`,
        position: [0.1, 0.6, 0],
        rotation: [0, THREE.MathUtils.degToRad(0), 0],
        flipped: true,
      },
    ],
    animationHotspot: {
      scale: 0.25,
      position: [0.2, 0.3, 1.4],
      rotation: [0, THREE.MathUtils.degToRad(0), 0],
      playLabel: "LINE CARD EJECTOR HANDLE",
      stopLabel: "CLOSE",
    },
  },
  {
    id: "yv4",
    title: "YOSEMITE V4",
    position: [-0.1, -1.15, -1],
    rotation: [0, THREE.MathUtils.degToRad(-90), 0],
    componentOpenPosition: [-0.1, -1.15, -1],
    scale: 0.11,
    path: `./assets/models/yv4/yv4.glb`,
    hotspots: [
      {
        scale: 2,
        title: `SIDE CAR CABLE`,
        description: `Sidecar Cable Sidecar cable is used only in configs that have a Sidecard expansion. This cable enables connection of 16 PCIe lanes and management signals between server and Sidecar expansion board.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-2.65, 4.1, 1.3],
        flipped: true,
      },
      {
        scale: 2,
        title: `NIC TRAY`,
        description: `NIC tray is a sub-assembly of Yosemite V4 chassis. The NIC tray consists of a Spider Board, a Medusa Board, a medusa management cable and NIC cables. The management board and NICs are front accessible. It is designed to be front serviceable and structurally sound to support the weight of all 8 blades.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-2.65, 3.1, 1.3],
        flipped: true,
      },
    ],
    components: [
      {
        path: `./assets/models/yv4/blade.glb`,
        id: "blade",
        title: "BLADE",
        position: [0, 0, 0],
        openPosition: [6.5, -3, -2.5],
        hotspots: [
          {
            scale: 7,
            title: `BLADE`,
            description: `The server blade design features a socketed higher power CPU, up to 12 channel DDR5 DIMMs, and a boot SSD. \n\nThe server board has larger front expansion which can be used for Flash drives or Accelerator modules or CXL memory modules. It also has a Sidecar and NIC connectors in the back. The Sidecar connector is used to add the Side card expansion over PCIe. The NIC connector sends PCIe lanes to NIC and management signals to BMC.`,
            rotation: [0, THREE.MathUtils.degToRad(90), 0],
            position: [-2, 2.2, 0.8],
            openPosition: [-2.5, 2.7, 0.9],
          },
        ],
      },
    ],
  },
  {
    id: "nic-tray",
    title: "NIC TRAY",
    position: [0, -1, -1.17],
    rotation: [0, THREE.MathUtils.degToRad(-90), 0],
    scale: 0.13,
    path: `./assets/models/yv4/nic-tray.glb`,
    hotspots: [
      {
        scale: 2,
        title: `OVERVIEW`,
        description: `NIC tray is a sub-assembly of Yosemite V4 chassis. The NIC tray consists of a Spider Board, a Medusa Board, a medusa management cable and NIC cables. The management board and NICs are front accessible. It is designed to be front serviceable and structurally sound to support the weight of all 8 blades.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-2.65, 3.6, -0.2],
        flipped: true,
      },
      {
        scale: 2,
        title: `MANAGEMENT BOARD`,
        description: `Management board houses the BMC and the Baseboard Storage Module(BSM). There is also a RTC with battery back up on the Management Board. The BMC on Management board manages everything in the chassis from servers to NICs to fans to hot swap controllers.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-2.65, 3.3, 1.25],
        flipped: true,
      },
      {
        scale: 2,
        title: `SPIDER BOARD`,
        description: `Spider board is heart of all interconnects. It connects the servers to NICs and Management board as well as Medusa, Fans to Management board. Different options for NIC sharing are enabled by modifying the Spider board.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-3.8, 2.9, 0.6],
        flipped: true,
      },
      {
        scale: 2,
        title: `MEDUSA BOARD`,
        description: `Medusa board is the main power board in the Yosemite v4 chassis. It manages the chassis hot plug into the rack, houses the 48V-12V converters, and supports the fans. It also has the hot swap controller for each of the blades.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-2, 2.9, -1],
        flipped: false,
      },
      {
        scale: 2,
        title: `NIC CABLE`,
        description: `This cable connects PCIe and management signals of the server blade to NIC and BMC respectively. It goes between server and Spider board. There are several types of NIC cables -  based on the NIC bandwidth required, sharing of NIC among servers and PCIe speed.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-2.5, 2.9, 0.6],
        flipped: false,
      },
      {
        scale: 2,
        title: `MEDUSA MANAGEMENT CABLE`,
        description: `This cable goes between the Medusa and Spider board. It enables BMC to manage the components on Medusa board as well as all the fans which are connected to Medusa board.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-3, 2.7, 1],
        flipped: true,
      },
      {
        scale: 2,
        title: `NIC`,
        description: `Yosemite v4 supports OCP 3.0 NIC. Both SFF and TSFF are supported. The NICs range from 100G to 800G NICs. The platform design enables NICs to be shared by servers.`,
        rotation: [0, THREE.MathUtils.degToRad(90), 0],
        position: [-2.1, 3.1, 1.25],
        flipped: false,
      },
    ],
    components: [],
  },
];

export default models;
